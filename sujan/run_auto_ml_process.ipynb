{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjGz3hVOoDmWBhTLJ7IjVL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pure-ML/PureMLApp/blob/main/sujan/run_auto_ml_process.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install FLAML"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XS6tqSJIeVoY",
        "outputId": "d218da57-c7f3-4b80-8b70-9ffb5b6c7793"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: FLAML in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Requirement already satisfied: NumPy>=1.17 in /usr/local/lib/python3.10/dist-packages (from FLAML) (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXgjW-ZBeIau",
        "outputId": "c7d511c2-1317-4816-db54-4ed6ba240d87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[flaml.automl.logger: 10-13 21:42:22] {1728} INFO - task = regression\n",
            "[flaml.automl.logger: 10-13 21:42:22] {1739} INFO - Evaluation method: cv\n",
            "[flaml.automl.logger: 10-13 21:42:22] {1838} INFO - Minimizing error metric: 1-r2\n",
            "[flaml.automl.logger: 10-13 21:42:22] {1955} INFO - List of ML learners in AutoML Run: ['xgboost']\n",
            "[flaml.automl.logger: 10-13 21:42:22] {2258} INFO - iteration 0, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:42:23] {2393} INFO - Estimated sufficient time budget=15638s. Estimated necessary time budget=16s.\n",
            "[flaml.automl.logger: 10-13 21:42:23] {2442} INFO -  at 1.6s,\testimator xgboost's best error=0.9339,\tbest estimator xgboost's best error=0.9339\n",
            "[flaml.automl.logger: 10-13 21:42:23] {2258} INFO - iteration 1, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:42:24] {2442} INFO -  at 2.1s,\testimator xgboost's best error=0.9339,\tbest estimator xgboost's best error=0.9339\n",
            "[flaml.automl.logger: 10-13 21:42:24] {2258} INFO - iteration 2, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:42:25] {2442} INFO -  at 3.1s,\testimator xgboost's best error=0.9339,\tbest estimator xgboost's best error=0.9339\n",
            "[flaml.automl.logger: 10-13 21:42:25] {2258} INFO - iteration 3, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:42:25] {2442} INFO -  at 3.6s,\testimator xgboost's best error=0.9339,\tbest estimator xgboost's best error=0.9339\n",
            "[flaml.automl.logger: 10-13 21:42:25] {2258} INFO - iteration 4, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:42:26] {2442} INFO -  at 4.1s,\testimator xgboost's best error=0.9339,\tbest estimator xgboost's best error=0.9339\n",
            "[flaml.automl.logger: 10-13 21:42:26] {2258} INFO - iteration 5, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:42:26] {2442} INFO -  at 4.6s,\testimator xgboost's best error=0.9339,\tbest estimator xgboost's best error=0.9339\n",
            "[flaml.automl.logger: 10-13 21:42:26] {2258} INFO - iteration 6, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:42:29] {2442} INFO -  at 6.9s,\testimator xgboost's best error=0.9339,\tbest estimator xgboost's best error=0.9339\n",
            "[flaml.automl.logger: 10-13 21:42:29] {2258} INFO - iteration 7, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:42:32] {2442} INFO -  at 10.4s,\testimator xgboost's best error=0.9339,\tbest estimator xgboost's best error=0.9339\n",
            "[flaml.automl.logger: 10-13 21:42:32] {2258} INFO - iteration 8, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:42:33] {2442} INFO -  at 11.2s,\testimator xgboost's best error=0.9339,\tbest estimator xgboost's best error=0.9339\n",
            "[flaml.automl.logger: 10-13 21:42:33] {2258} INFO - iteration 9, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:42:33] {2442} INFO -  at 11.8s,\testimator xgboost's best error=0.9121,\tbest estimator xgboost's best error=0.9121\n",
            "[flaml.automl.logger: 10-13 21:42:33] {2258} INFO - iteration 10, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:42:34] {2442} INFO -  at 12.3s,\testimator xgboost's best error=0.9121,\tbest estimator xgboost's best error=0.9121\n",
            "[flaml.automl.logger: 10-13 21:42:34] {2258} INFO - iteration 11, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:42:35] {2442} INFO -  at 13.2s,\testimator xgboost's best error=0.8807,\tbest estimator xgboost's best error=0.8807\n",
            "[flaml.automl.logger: 10-13 21:42:35] {2258} INFO - iteration 12, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:42:36] {2442} INFO -  at 14.5s,\testimator xgboost's best error=0.8807,\tbest estimator xgboost's best error=0.8807\n",
            "[flaml.automl.logger: 10-13 21:42:36] {2685} INFO - retrain xgboost for 0.0s\n",
            "[flaml.automl.logger: 10-13 21:42:36] {2688} INFO - retrained model: XGBRegressor(base_score=None, booster=None, callbacks=[],\n",
            "             colsample_bylevel=0.9559873420149136, colsample_bynode=None,\n",
            "             colsample_bytree=1.0, device=None, early_stopping_rounds=None,\n",
            "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "             gamma=None, grow_policy='lossguide', importance_type=None,\n",
            "             interaction_constraints=None, learning_rate=0.18989872471196662,\n",
            "             max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "             max_delta_step=None, max_depth=0, max_leaves=7,\n",
            "             min_child_weight=0.281435598387862, missing=nan,\n",
            "             monotone_constraints=None, multi_strategy=None, n_estimators=4,\n",
            "             n_jobs=-1, num_parallel_tree=None, random_state=None, ...)\n",
            "[flaml.automl.logger: 10-13 21:42:36] {1985} INFO - fit succeeded\n",
            "[flaml.automl.logger: 10-13 21:42:36] {1986} INFO - Time taken to find the best model: 13.21974515914917\n",
            "Best Hyperparameters for XGBoost: {'n_estimators': 4, 'max_leaves': 7, 'min_child_weight': 0.281435598387862, 'learning_rate': 0.18989872471196662, 'subsample': 0.8286310106576346, 'colsample_bylevel': 0.9559873420149136, 'colsample_bytree': 1.0, 'reg_alpha': 0.07078541090409908, 'reg_lambda': 14.583470334911325}\n",
            "R-squared on test data: 0.04301398992538452\n",
            "Mean Squared Error on test data: 2064898590.168775\n",
            "Mean Absolute Error on test data: 22881.319619140624\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from flaml import AutoML\n",
        "\n",
        "# Load the dataset and take the first 1000 rows\n",
        "data = pd.read_csv('/content/sample_data/base_dataset.csv').head(1000)\n",
        "\n",
        "# Separate the features and the target variable\n",
        "X = data.drop(columns=['price'])  # Drop the target column from the features\n",
        "y = data['price']  # Set the target column\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set up FLAML AutoML to use only XGBoost\n",
        "automl = AutoML()\n",
        "automl_settings = {\n",
        "    \"time_budget\": 15,  # Set the time budget in seconds\n",
        "    \"metric\": 'r2',  # For regression tasks, R-squared is often a good metric\n",
        "    \"task\": 'regression',  # Specify that this is a regression task\n",
        "    \"estimator_list\": [\"xgboost\"],  # Use only XGBoost for model selection\n",
        "}\n",
        "\n",
        "# Train the XGBoost model using AutoML\n",
        "automl.fit(X_train=X_train, y_train=y_train, **automl_settings)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = automl.predict(X_test)\n",
        "\n",
        "# Calculate performance metrics\n",
        "mse = mean_squared_error(y_test, y_pred)  # Mean Squared Error\n",
        "mae = mean_absolute_error(y_test, y_pred)  # Mean Absolute Error\n",
        "r2 = r2_score(y_test, y_pred)  # R-squared Score\n",
        "\n",
        "# Output the model and performance metrics\n",
        "print(\"Best Hyperparameters for XGBoost:\", automl.best_config)\n",
        "print(\"R-squared on test data:\", r2)\n",
        "print(\"Mean Squared Error on test data:\", mse)\n",
        "print(\"Mean Absolute Error on test data:\", mae)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JAao5ZuNg3Bc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K1k4ADmj0fED"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "null_agent_output=pd.read_csv(\"/content/sample_data/null_agent_output_dataset.csv\")\n",
        "\n",
        "column_agent_output=pd.read_csv(\"/content/sample_data/column_agent_output.csv\")\n",
        "column_agent_output.drop(columns=['body_type', 'luxury_vehicle'])\n",
        "column_agent_output.to_csv(\"/content/sample_data/column_agent_output.csv\", index=False)\n",
        "\n",
        "pavel_agent_output=pd.read_csv(\"/content/sample_data/pavel_agent_output_new.csv\")\n",
        "\n",
        "merged_df_step1 = null_agent_output.combine_first(column_agent_output)\n",
        "\n",
        "final_merged_df = merged_df_step1.combine_first(pavel_agent_output)\n",
        "\n",
        "final_merged_df_filled = final_merged_df.fillna('')\n",
        "\n",
        "final_merged_df_filled.head()\n",
        "\n",
        "output_path = '/content/sample_data/datasmith.csv'\n",
        "\n",
        "final_merged_df_filled.to_csv(output_path, index=False)"
      ],
      "metadata": {
        "id": "THQA9ptTnltr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from flaml import AutoML\n",
        "\n",
        "# Load the dataset and take the first 1000 rows\n",
        "data = pd.read_csv('/content/sample_data/datasmith.csv').head(1000)\n",
        "\n",
        "# Separate the features and the target variable\n",
        "X = data.drop(columns=['price'])  # Drop the target column from the features\n",
        "y = data['price']  # Set the target column\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set up FLAML AutoML to use only XGBoost\n",
        "automl = AutoML()\n",
        "automl_settings = {\n",
        "    \"time_budget\": 15,  # Set the time budget in seconds\n",
        "    \"metric\": 'r2',  # For regression tasks, R-squared is often a good metric\n",
        "    \"task\": 'regression',  # Specify that this is a regression task\n",
        "    \"estimator_list\": [\"xgboost\"],  # Use only XGBoost for model selection\n",
        "}\n",
        "\n",
        "# Train the XGBoost model using AutoML\n",
        "automl.fit(X_train=X_train, y_train=y_train, **automl_settings)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = automl.predict(X_test)\n",
        "\n",
        "# Calculate performance metrics\n",
        "mse = mean_squared_error(y_test, y_pred)  # Mean Squared Error\n",
        "mae = mean_absolute_error(y_test, y_pred)  # Mean Absolute Error\n",
        "r2 = r2_score(y_test, y_pred)  # R-squared Score\n",
        "\n",
        "# Output the model and performance metrics\n",
        "print(\"Best Hyperparameters for XGBoost:\", automl.best_config)\n",
        "print(\"R-squared on test data:\", r2)\n",
        "print(\"Mean Squared Error on test data:\", mse)\n",
        "print(\"Mean Absolute Error on test data:\", mae)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Se-sOhcPo2B2",
        "outputId": "deb22d36-ec26-40cf-b23b-cdd5d03e3025"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[flaml.automl.logger: 10-13 21:42:38] {1728} INFO - task = regression\n",
            "[flaml.automl.logger: 10-13 21:42:38] {1739} INFO - Evaluation method: cv\n",
            "[flaml.automl.logger: 10-13 21:42:38] {1838} INFO - Minimizing error metric: 1-r2\n",
            "[flaml.automl.logger: 10-13 21:42:38] {1955} INFO - List of ML learners in AutoML Run: ['xgboost']\n",
            "[flaml.automl.logger: 10-13 21:42:38] {2258} INFO - iteration 0, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:42:39] {2393} INFO - Estimated sufficient time budget=6194s. Estimated necessary time budget=6s.\n",
            "[flaml.automl.logger: 10-13 21:42:39] {2442} INFO -  at 0.7s,\testimator xgboost's best error=0.9955,\tbest estimator xgboost's best error=0.9955\n",
            "[flaml.automl.logger: 10-13 21:42:39] {2258} INFO - iteration 1, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:42:40] {2442} INFO -  at 1.9s,\testimator xgboost's best error=0.9955,\tbest estimator xgboost's best error=0.9955\n",
            "[flaml.automl.logger: 10-13 21:42:40] {2258} INFO - iteration 2, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:42:42] {2442} INFO -  at 4.0s,\testimator xgboost's best error=0.9955,\tbest estimator xgboost's best error=0.9955\n",
            "[flaml.automl.logger: 10-13 21:42:42] {2258} INFO - iteration 3, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:42:44] {2442} INFO -  at 6.0s,\testimator xgboost's best error=0.9955,\tbest estimator xgboost's best error=0.9955\n",
            "[flaml.automl.logger: 10-13 21:42:44] {2258} INFO - iteration 4, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:42:45] {2442} INFO -  at 6.6s,\testimator xgboost's best error=0.9955,\tbest estimator xgboost's best error=0.9955\n",
            "[flaml.automl.logger: 10-13 21:42:45] {2258} INFO - iteration 5, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:42:46] {2442} INFO -  at 7.8s,\testimator xgboost's best error=0.9955,\tbest estimator xgboost's best error=0.9955\n",
            "[flaml.automl.logger: 10-13 21:42:46] {2258} INFO - iteration 6, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:42:48] {2442} INFO -  at 9.3s,\testimator xgboost's best error=0.9955,\tbest estimator xgboost's best error=0.9955\n",
            "[flaml.automl.logger: 10-13 21:42:48] {2258} INFO - iteration 7, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:42:48] {2442} INFO -  at 9.8s,\testimator xgboost's best error=0.9955,\tbest estimator xgboost's best error=0.9955\n",
            "[flaml.automl.logger: 10-13 21:42:48] {2258} INFO - iteration 8, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:42:49] {2442} INFO -  at 10.4s,\testimator xgboost's best error=0.9955,\tbest estimator xgboost's best error=0.9955\n",
            "[flaml.automl.logger: 10-13 21:42:49] {2258} INFO - iteration 9, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:42:50] {2442} INFO -  at 11.5s,\testimator xgboost's best error=0.9231,\tbest estimator xgboost's best error=0.9231\n",
            "[flaml.automl.logger: 10-13 21:42:50] {2258} INFO - iteration 10, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:42:50] {2442} INFO -  at 12.1s,\testimator xgboost's best error=0.9231,\tbest estimator xgboost's best error=0.9231\n",
            "[flaml.automl.logger: 10-13 21:42:50] {2258} INFO - iteration 11, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:42:53] {2442} INFO -  at 14.1s,\testimator xgboost's best error=0.9231,\tbest estimator xgboost's best error=0.9231\n",
            "[flaml.automl.logger: 10-13 21:42:53] {2685} INFO - retrain xgboost for 0.1s\n",
            "[flaml.automl.logger: 10-13 21:42:53] {2688} INFO - retrained model: XGBRegressor(base_score=None, booster=None, callbacks=[], colsample_bylevel=1.0,\n",
            "             colsample_bynode=None, colsample_bytree=0.9468117873770695,\n",
            "             device=None, early_stopping_rounds=None, enable_categorical=False,\n",
            "             eval_metric=None, feature_types=None, gamma=None,\n",
            "             grow_policy='lossguide', importance_type=None,\n",
            "             interaction_constraints=None, learning_rate=0.12179245439397736,\n",
            "             max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "             max_delta_step=None, max_depth=0, max_leaves=8,\n",
            "             min_child_weight=1.4414106781003007, missing=nan,\n",
            "             monotone_constraints=None, multi_strategy=None, n_estimators=4,\n",
            "             n_jobs=-1, num_parallel_tree=None, random_state=None, ...)\n",
            "[flaml.automl.logger: 10-13 21:42:53] {1985} INFO - fit succeeded\n",
            "[flaml.automl.logger: 10-13 21:42:53] {1986} INFO - Time taken to find the best model: 11.506625175476074\n",
            "Best Hyperparameters for XGBoost: {'n_estimators': 4, 'max_leaves': 8, 'min_child_weight': 1.4414106781003007, 'learning_rate': 0.12179245439397736, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9468117873770695, 'reg_alpha': 0.025118956715098555, 'reg_lambda': 5.047693393974604}\n",
            "R-squared on test data: 0.04853123426437378\n",
            "Mean Squared Error on test data: 2052993910.3918276\n",
            "Mean Absolute Error on test data: 24473.31412109375\n"
          ]
        }
      ]
    }
  ]
}