{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjGz3hVOoDmWBhTLJ7IjVL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pure-ML/PureMLApp/blob/main/sujan/run_auto_ml_process.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install FLAML"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XS6tqSJIeVoY",
        "outputId": "e06e8487-5118-481f-9e49-f05385f18a40"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: FLAML in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Requirement already satisfied: NumPy>=1.17 in /usr/local/lib/python3.10/dist-packages (from FLAML) (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXgjW-ZBeIau",
        "outputId": "19163a87-6f99-4f06-d5d0-72523f987366"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[flaml.automl.logger: 10-13 21:44:13] {1728} INFO - task = regression\n",
            "[flaml.automl.logger: 10-13 21:44:13] {1739} INFO - Evaluation method: cv\n",
            "[flaml.automl.logger: 10-13 21:44:13] {1838} INFO - Minimizing error metric: 1-r2\n",
            "[flaml.automl.logger: 10-13 21:44:13] {1955} INFO - List of ML learners in AutoML Run: ['xgboost']\n",
            "[flaml.automl.logger: 10-13 21:44:13] {2258} INFO - iteration 0, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:44:14] {2393} INFO - Estimated sufficient time budget=8769s. Estimated necessary time budget=9s.\n",
            "[flaml.automl.logger: 10-13 21:44:14] {2442} INFO -  at 1.0s,\testimator xgboost's best error=0.9339,\tbest estimator xgboost's best error=0.9339\n",
            "[flaml.automl.logger: 10-13 21:44:14] {2258} INFO - iteration 1, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:44:14] {2442} INFO -  at 1.3s,\testimator xgboost's best error=0.9339,\tbest estimator xgboost's best error=0.9339\n",
            "[flaml.automl.logger: 10-13 21:44:14] {2258} INFO - iteration 2, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:44:14] {2442} INFO -  at 1.8s,\testimator xgboost's best error=0.9339,\tbest estimator xgboost's best error=0.9339\n",
            "[flaml.automl.logger: 10-13 21:44:14] {2258} INFO - iteration 3, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:44:15] {2442} INFO -  at 2.1s,\testimator xgboost's best error=0.9339,\tbest estimator xgboost's best error=0.9339\n",
            "[flaml.automl.logger: 10-13 21:44:15] {2258} INFO - iteration 4, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:44:16] {2442} INFO -  at 3.5s,\testimator xgboost's best error=0.9339,\tbest estimator xgboost's best error=0.9339\n",
            "[flaml.automl.logger: 10-13 21:44:16] {2258} INFO - iteration 5, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:44:17] {2442} INFO -  at 4.0s,\testimator xgboost's best error=0.9339,\tbest estimator xgboost's best error=0.9339\n",
            "[flaml.automl.logger: 10-13 21:44:17] {2258} INFO - iteration 6, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:44:18] {2442} INFO -  at 4.9s,\testimator xgboost's best error=0.9339,\tbest estimator xgboost's best error=0.9339\n",
            "[flaml.automl.logger: 10-13 21:44:18] {2258} INFO - iteration 7, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:44:18] {2442} INFO -  at 5.7s,\testimator xgboost's best error=0.9339,\tbest estimator xgboost's best error=0.9339\n",
            "[flaml.automl.logger: 10-13 21:44:18] {2258} INFO - iteration 8, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:44:19] {2442} INFO -  at 6.6s,\testimator xgboost's best error=0.9339,\tbest estimator xgboost's best error=0.9339\n",
            "[flaml.automl.logger: 10-13 21:44:19] {2258} INFO - iteration 9, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:44:21] {2442} INFO -  at 8.3s,\testimator xgboost's best error=0.9121,\tbest estimator xgboost's best error=0.9121\n",
            "[flaml.automl.logger: 10-13 21:44:21] {2258} INFO - iteration 10, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:44:22] {2442} INFO -  at 8.9s,\testimator xgboost's best error=0.9121,\tbest estimator xgboost's best error=0.9121\n",
            "[flaml.automl.logger: 10-13 21:44:22] {2258} INFO - iteration 11, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:44:24] {2442} INFO -  at 11.1s,\testimator xgboost's best error=0.8920,\tbest estimator xgboost's best error=0.8920\n",
            "[flaml.automl.logger: 10-13 21:44:24] {2258} INFO - iteration 12, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:44:27] {2442} INFO -  at 14.0s,\testimator xgboost's best error=0.8920,\tbest estimator xgboost's best error=0.8920\n",
            "[flaml.automl.logger: 10-13 21:44:27] {2685} INFO - retrain xgboost for 0.1s\n",
            "[flaml.automl.logger: 10-13 21:44:27] {2688} INFO - retrained model: XGBRegressor(base_score=None, booster=None, callbacks=[],\n",
            "             colsample_bylevel=0.9559873420149136, colsample_bynode=None,\n",
            "             colsample_bytree=1.0, device=None, early_stopping_rounds=None,\n",
            "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "             gamma=None, grow_policy='lossguide', importance_type=None,\n",
            "             interaction_constraints=None, learning_rate=0.18989872471196662,\n",
            "             max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "             max_delta_step=None, max_depth=0, max_leaves=7,\n",
            "             min_child_weight=0.281435598387862, missing=nan,\n",
            "             monotone_constraints=None, multi_strategy=None, n_estimators=4,\n",
            "             n_jobs=-1, num_parallel_tree=None, random_state=None, ...)\n",
            "[flaml.automl.logger: 10-13 21:44:27] {1985} INFO - fit succeeded\n",
            "[flaml.automl.logger: 10-13 21:44:27] {1986} INFO - Time taken to find the best model: 11.126497983932495\n",
            "Best Hyperparameters for XGBoost: {'n_estimators': 4, 'max_leaves': 7, 'min_child_weight': 0.281435598387862, 'learning_rate': 0.18989872471196662, 'subsample': 0.8286310106576346, 'colsample_bylevel': 0.9559873420149136, 'colsample_bytree': 1.0, 'reg_alpha': 0.07078541090409908, 'reg_lambda': 14.583470334911325}\n",
            "R-squared on test data: 0.04301398992538452\n",
            "Mean Squared Error on test data: 2064898590.168775\n",
            "Mean Absolute Error on test data: 22881.319619140624\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from flaml import AutoML\n",
        "\n",
        "# Load the dataset and take the first 1000 rows\n",
        "data = pd.read_csv('/content/sample_data/base_dataset.csv').head(1000)\n",
        "\n",
        "# Separate the features and the target variable\n",
        "X = data.drop(columns=['price'])  # Drop the target column from the features\n",
        "y = data['price']  # Set the target column\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set up FLAML AutoML to use only XGBoost\n",
        "automl = AutoML()\n",
        "automl_settings = {\n",
        "    \"time_budget\": 15,  # Set the time budget in seconds\n",
        "    \"metric\": 'r2',  # For regression tasks, R-squared is often a good metric\n",
        "    \"task\": 'regression',  # Specify that this is a regression task\n",
        "    \"estimator_list\": [\"xgboost\"],  # Use only XGBoost for model selection\n",
        "}\n",
        "\n",
        "# Train the XGBoost model using AutoML\n",
        "automl.fit(X_train=X_train, y_train=y_train, **automl_settings)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = automl.predict(X_test)\n",
        "\n",
        "# Calculate performance metrics\n",
        "mse = mean_squared_error(y_test, y_pred)  # Mean Squared Error\n",
        "mae = mean_absolute_error(y_test, y_pred)  # Mean Absolute Error\n",
        "r2 = r2_score(y_test, y_pred)  # R-squared Score\n",
        "\n",
        "# Output the model and performance metrics\n",
        "print(\"Best Hyperparameters for XGBoost:\", automl.best_config)\n",
        "print(\"R-squared on test data:\", r2)\n",
        "print(\"Mean Squared Error on test data:\", mse)\n",
        "print(\"Mean Absolute Error on test data:\", mae)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JAao5ZuNg3Bc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K1k4ADmj0fED"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "null_agent_output=pd.read_csv(\"/content/sample_data/null_agent_output_dataset.csv\")\n",
        "\n",
        "column_agent_output=pd.read_csv(\"/content/sample_data/column_agent_output.csv\")\n",
        "column_agent_output.drop(columns=['body_type', 'luxury_vehicle'])\n",
        "column_agent_output.to_csv(\"/content/sample_data/column_agent_output.csv\", index=False)\n",
        "\n",
        "pavel_agent_output=pd.read_csv(\"/content/sample_data/pavel_agent_output_new.csv\")\n",
        "\n",
        "merged_df_step1 = null_agent_output.combine_first(column_agent_output)\n",
        "\n",
        "final_merged_df = merged_df_step1.combine_first(pavel_agent_output)\n",
        "\n",
        "final_merged_df_filled = final_merged_df.fillna('')\n",
        "\n",
        "final_merged_df_filled.head()\n",
        "\n",
        "output_path = '/content/sample_data/datasmith.csv'\n",
        "\n",
        "final_merged_df_filled.to_csv(output_path, index=False)"
      ],
      "metadata": {
        "id": "THQA9ptTnltr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from flaml import AutoML\n",
        "\n",
        "# Load the dataset and take the first 1000 rows\n",
        "data = pd.read_csv('/content/sample_data/datasmith.csv').head(1000)\n",
        "\n",
        "# Separate the features and the target variable\n",
        "X = data.drop(columns=['price'])  # Drop the target column from the features\n",
        "y = data['price']  # Set the target column\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set up FLAML AutoML to use only XGBoost\n",
        "automl = AutoML()\n",
        "automl_settings = {\n",
        "    \"time_budget\": 15,  # Set the time budget in seconds\n",
        "    \"metric\": 'r2',  # For regression tasks, R-squared is often a good metric\n",
        "    \"task\": 'regression',  # Specify that this is a regression task\n",
        "    \"estimator_list\": [\"xgboost\"],  # Use only XGBoost for model selection\n",
        "}\n",
        "\n",
        "# Train the XGBoost model using AutoML\n",
        "automl.fit(X_train=X_train, y_train=y_train, **automl_settings)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = automl.predict(X_test)\n",
        "\n",
        "# Calculate performance metrics\n",
        "mse = mean_squared_error(y_test, y_pred)  # Mean Squared Error\n",
        "mae = mean_absolute_error(y_test, y_pred)  # Mean Absolute Error\n",
        "r2 = r2_score(y_test, y_pred)  # R-squared Score\n",
        "\n",
        "# Output the model and performance metrics\n",
        "print(\"Best Hyperparameters for XGBoost:\", automl.best_config)\n",
        "print(\"R-squared on test data:\", r2)\n",
        "print(\"Mean Squared Error on test data:\", mse)\n",
        "print(\"Mean Absolute Error on test data:\", mae)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Se-sOhcPo2B2",
        "outputId": "342f0b61-0837-4428-fc38-c122d1695dc9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[flaml.automl.logger: 10-13 21:44:29] {1728} INFO - task = regression\n",
            "[flaml.automl.logger: 10-13 21:44:29] {1739} INFO - Evaluation method: cv\n",
            "[flaml.automl.logger: 10-13 21:44:29] {1838} INFO - Minimizing error metric: 1-r2\n",
            "[flaml.automl.logger: 10-13 21:44:29] {1955} INFO - List of ML learners in AutoML Run: ['xgboost']\n",
            "[flaml.automl.logger: 10-13 21:44:29] {2258} INFO - iteration 0, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:44:30] {2393} INFO - Estimated sufficient time budget=8122s. Estimated necessary time budget=8s.\n",
            "[flaml.automl.logger: 10-13 21:44:30] {2442} INFO -  at 0.9s,\testimator xgboost's best error=0.9955,\tbest estimator xgboost's best error=0.9955\n",
            "[flaml.automl.logger: 10-13 21:44:30] {2258} INFO - iteration 1, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:44:30] {2442} INFO -  at 1.4s,\testimator xgboost's best error=0.9955,\tbest estimator xgboost's best error=0.9955\n",
            "[flaml.automl.logger: 10-13 21:44:30] {2258} INFO - iteration 2, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:44:31] {2442} INFO -  at 2.1s,\testimator xgboost's best error=0.9955,\tbest estimator xgboost's best error=0.9955\n",
            "[flaml.automl.logger: 10-13 21:44:31] {2258} INFO - iteration 3, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:44:32] {2442} INFO -  at 3.1s,\testimator xgboost's best error=0.9955,\tbest estimator xgboost's best error=0.9955\n",
            "[flaml.automl.logger: 10-13 21:44:32] {2258} INFO - iteration 4, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:44:33] {2442} INFO -  at 4.3s,\testimator xgboost's best error=0.9955,\tbest estimator xgboost's best error=0.9955\n",
            "[flaml.automl.logger: 10-13 21:44:33] {2258} INFO - iteration 5, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:44:34] {2442} INFO -  at 4.9s,\testimator xgboost's best error=0.9955,\tbest estimator xgboost's best error=0.9955\n",
            "[flaml.automl.logger: 10-13 21:44:34] {2258} INFO - iteration 6, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:44:35] {2442} INFO -  at 5.7s,\testimator xgboost's best error=0.9955,\tbest estimator xgboost's best error=0.9955\n",
            "[flaml.automl.logger: 10-13 21:44:35] {2258} INFO - iteration 7, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:44:36] {2442} INFO -  at 6.7s,\testimator xgboost's best error=0.9955,\tbest estimator xgboost's best error=0.9955\n",
            "[flaml.automl.logger: 10-13 21:44:36] {2258} INFO - iteration 8, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:44:37] {2442} INFO -  at 7.5s,\testimator xgboost's best error=0.9955,\tbest estimator xgboost's best error=0.9955\n",
            "[flaml.automl.logger: 10-13 21:44:37] {2258} INFO - iteration 9, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:44:38] {2442} INFO -  at 8.8s,\testimator xgboost's best error=0.9231,\tbest estimator xgboost's best error=0.9231\n",
            "[flaml.automl.logger: 10-13 21:44:38] {2258} INFO - iteration 10, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:44:40] {2442} INFO -  at 11.1s,\testimator xgboost's best error=0.9231,\tbest estimator xgboost's best error=0.9231\n",
            "[flaml.automl.logger: 10-13 21:44:40] {2258} INFO - iteration 11, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:44:41] {2442} INFO -  at 12.4s,\testimator xgboost's best error=0.8830,\tbest estimator xgboost's best error=0.8830\n",
            "[flaml.automl.logger: 10-13 21:44:41] {2258} INFO - iteration 12, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:44:42] {2442} INFO -  at 13.3s,\testimator xgboost's best error=0.8830,\tbest estimator xgboost's best error=0.8830\n",
            "[flaml.automl.logger: 10-13 21:44:42] {2258} INFO - iteration 13, current learner xgboost\n",
            "[flaml.automl.logger: 10-13 21:44:43] {2442} INFO -  at 13.7s,\testimator xgboost's best error=0.8830,\tbest estimator xgboost's best error=0.8830\n",
            "[flaml.automl.logger: 10-13 21:44:43] {2685} INFO - retrain xgboost for 0.2s\n",
            "[flaml.automl.logger: 10-13 21:44:43] {2688} INFO - retrained model: XGBRegressor(base_score=None, booster=None, callbacks=[],\n",
            "             colsample_bylevel=0.9559873420149136, colsample_bynode=None,\n",
            "             colsample_bytree=1.0, device=None, early_stopping_rounds=None,\n",
            "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "             gamma=None, grow_policy='lossguide', importance_type=None,\n",
            "             interaction_constraints=None, learning_rate=0.18989872471196662,\n",
            "             max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "             max_delta_step=None, max_depth=0, max_leaves=7,\n",
            "             min_child_weight=0.281435598387862, missing=nan,\n",
            "             monotone_constraints=None, multi_strategy=None, n_estimators=4,\n",
            "             n_jobs=-1, num_parallel_tree=None, random_state=None, ...)\n",
            "[flaml.automl.logger: 10-13 21:44:43] {1985} INFO - fit succeeded\n",
            "[flaml.automl.logger: 10-13 21:44:43] {1986} INFO - Time taken to find the best model: 12.405307292938232\n",
            "Best Hyperparameters for XGBoost: {'n_estimators': 4, 'max_leaves': 7, 'min_child_weight': 0.281435598387862, 'learning_rate': 0.18989872471196662, 'subsample': 0.8286310106576346, 'colsample_bylevel': 0.9559873420149136, 'colsample_bytree': 1.0, 'reg_alpha': 0.07078541090409908, 'reg_lambda': 14.583470334911325}\n",
            "R-squared on test data: 0.08346879482269287\n",
            "Mean Squared Error on test data: 1977608585.7961793\n",
            "Mean Absolute Error on test data: 23110.0653125\n"
          ]
        }
      ]
    }
  ]
}